njobs <- length(st)
rtsum.dist <- subset(df2, stats.attr=="bot_runtime_sum" & Cluster==rtsum.clusters[cl.i])
d <- as.character(rtsum.dist$distribution)
rtsum <- vector()
jobs.len <- 0
while (jobs.len < njobs) {
if (d != "exponential")
rtsum <- c(rtsum, transform.f(dists.rfun[[d]](buffer, rtsum.dist$Param1,
rtsum.dist$Param2)))
else
rtsum <- c(rtsum, transform.f(dists.rfun[[d]](buffer, rtsum.dist$Param1)))
rtsum <- rtsum[rtsum <= rt.max]
jobs.len <- length(rtsum)
}
length(rtsum) <- njobs
ntasks <- vector()
rttask.dist <- subset(df2, stats.attr=="task_runtime" & Cluster==trt.clusters[cl.i])
d <- as.character(rttask.dist$distribution)
all_rt_tasks <- foreach(bot_runtime=rtsum, .combine=c) %do% {
agg_runtime <- 0
job_size <- 0
rt_tasks <- vector(mode="numeric")
task.i <- 1
loop <- TRUE
while(loop) {
if (d != "exponential")
rt_tasks <- c(rt_tasks, transform.f(dists.rfun[[d]](buffer, rttask.dist$Param1,
rttask.dist$Param2)))
else
rt_tasks <- c(rt_tasks, transform.f(dists.rfun[[d]](buffer, rttask.dist$Param1)))
rt.sum <- cumsum(rt_tasks[(job_size+1):length(rt_tasks)]) + agg_runtime
task.i <- max(0, min(which(rt.sum >= bot_runtime))-1)
if (!is.infinite(task.i)) {
length(rt_tasks) <- job_size + task.i + 1
rt_tasks[length(rt_tasks)] <- ifelse(task.i > 0, bot_runtime-rt.sum[task.i],
bot_runtime-agg_runtime)
agg_runtime <- bot_runtime
loop <- FALSE
} else {
agg_runtime <- rt.sum[length(rt.sum)]
}
job_size <- length(rt_tasks)
}
ntasks <- c(ntasks,job_size)
return(rt_tasks)
}
uid <- paste("U",u.i,sep="")
pid <- paste("P",p.i,sep="")
iat.cid <- paste("C",iat.clusters[cl.i],sep="")
rtsum.cid <- paste("C",rtsum.clusters[cl.i],sep="")
rttask.cid <- paste("C", trt.clusters[cl.i],sep="")
if (reduced.wl) {
wl <- subset(data.frame(SubmitTime=round(st), RunTime.sum=round(rtsum),
JobID=1:length(ntasks), BoTSize=ntasks, UserID=uid, PeerID=pid,
TraceID=trace, Cluster.IAT=iat.cid, Cluster.JRT=rtsum.cid,
Cluster.TRT=rttask.cid), SubmitTime <= duration.secs)
} else {
wl <- subset(data.frame(SubmitTime=rep(round(st),ntasks), RunTime=round(all_rt_tasks),
JobID=rep(1:length(ntasks),ntasks), UserID=uid, PeerID=pid,
TraceID=trace, Cluster.IAT=iat.cid, Cluster.JRT=rtsum.cid,
Cluster.TRT=rttask.cid), SubmitTime <= duration.secs)
}
write.table(wl, paste('workload_clust_', sites.per.trace, 'spt_', users.per.site, "ups_",
trace,'.txt',sep=""), col.names=is.first, append=!is.first,
quote=FALSE, row.names=FALSE)
is.first <- FALSE
}
}
}
}
GenerateWorkload()
#################################################################################################
#
# Tools for the synthetic workload generation process proposed in the paper:
#
# - Marcus Carvalho, Francisco Brasileiro.
#   Modelagem da Carga de Trabalho de Grades Computacionais Baseada no Comportamento dos UsuÃ¡rios.
#   Submitted to: SimpÃ³sio Brasileiro de Redes de Computadores e Sistemas DistribuÃ­dos (SBRC), 2012.
#
# Last update: 05-Dec-2011
#
#################################################################################################
##
# Generates the synthetic workload using probability distributions listed in the input file.
#
# Inputs:
# - file.dits: input file generated by the model extraction process, with a list of distributions
#              for each attribute and trace.
# - k: number of clusters (groups) for each attribute
# - sites.per.trace: number of sites to create in the synthetic workload for each trace used
#                    in the model
# - users.per.trace: number os users that each created site comprises.
# - duration.sec: the duration of the synthetic workload in seconds.
# - rt.max: limit for the maximum runtime for a job (sum of tasks runtime for a job)
# - reduced.wl: if true, generates a job level workload ommiting task level information. Used if
#               a lower storage footprint is needed and high level information is enough.
# - transform.f: transformation function on the data generated. For instance, if a base-2 log
#                transformation was used in the modeling phase, a base-2 exponential function
#                is needed in the workload generation.
##
GenerateWorkload <- function(file.dists="./wl_fitdist_best.txt", k=5, sites.per.trace=10,
users.per.site=10, duration.secs=3600*24*3, rt.max=3600*24*3,
reduced.wl=FALSE, transform.f=function(x) 2^x) {
require(foreach)
require(doMC)
registerDoMC()
dists.rfun <- list(normal=rnorm, lognormal=rlnorm, gamma=rgamma, exponential=rexp,
weibull=rweibull)
dists.qfun <- list(normal=qnorm, lognormal=qlnorm, gamma=qgamma, exponential=qexp,
weibull=qweibull)
buffer <- 10
df <- read.table(file.dists, header=T)
df <- subset(df, k == 5)
p.i <- 0
u.i <- 0
for(trace in unique(df$Trace)) {
is.first <- TRUE
iat.dist <- subset(df, stats.attr=="bot_user_iat" & Trace==trace)
iat.clusters <- sample(iat.dist$Cluster, sites.per.trace*users.per.site,
prob=iat.dist$Fraction, replace=TRUE)
rtsum.dist <- subset(df, stats.attr=="bot_runtime_sum" & Trace==trace)
rtsum.clusters <- sample(rtsum.dist$Cluster, sites.per.trace*users.per.site,
prob=rtsum.dist$Fraction, replace=TRUE)
trt.dist <- subset(df, stats.attr=="task_runtime" & Trace==trace)
trt.clusters <- sample(trt.dist$Cluster, sites.per.trace*users.per.site,
prob=trt.dist$Fraction, replace=TRUE)
cl.i <- 0
for(peer in 1:sites.per.trace) {
p.i <- p.i + 1
for(user in 1:users.per.site) {
gc()
cl.i <- cl.i + 1
u.i <- u.i + 1
df2 <- subset(df, Trace == trace)
iat.dist <- subset(df2, stats.attr=="bot_user_iat" & Cluster==iat.clusters[cl.i])
d <- as.character(iat.dist$distribution)
t <- 0
iat <- vector()
st <- vector()
while (t < duration.secs) {
if (d != "exponential")
iat <- c(iat, transform.f(dists.rfun[[d]](buffer, iat.dist$Param1, iat.dist$Param2)))
else
iat <- c(iat, transform.f(dists.rfun[[d]](buffer, iat.dist$Param1)))
st <- cumsum(iat)
t <- st[length(st)]
}
njobs <- length(st)
rtsum.dist <- subset(df2, stats.attr=="bot_runtime_sum" & Cluster==rtsum.clusters[cl.i])
d <- as.character(rtsum.dist$distribution)
rtsum <- vector()
jobs.len <- 0
while (jobs.len < njobs) {
if (d != "exponential")
rtsum <- c(rtsum, transform.f(dists.rfun[[d]](buffer, rtsum.dist$Param1,
rtsum.dist$Param2)))
else
rtsum <- c(rtsum, transform.f(dists.rfun[[d]](buffer, rtsum.dist$Param1)))
rtsum <- rtsum[rtsum <= rt.max]
jobs.len <- length(rtsum)
}
length(rtsum) <- njobs
ntasks <- vector()
rttask.dist <- subset(df2, stats.attr=="task_runtime" & Cluster==trt.clusters[cl.i])
d <- as.character(rttask.dist$distribution)
all_rt_tasks <- foreach(bot_runtime=rtsum, .combine=c) %do% {
agg_runtime <- 0
job_size <- 0
rt_tasks <- vector(mode="numeric")
task.i <- 1
loop <- TRUE
while(loop) {
if (d != "exponential")
rt_tasks <- c(rt_tasks, transform.f(dists.rfun[[d]](buffer, rttask.dist$Param1,
rttask.dist$Param2)))
else
rt_tasks <- c(rt_tasks, transform.f(dists.rfun[[d]](buffer, rttask.dist$Param1)))
rt.sum <- cumsum(rt_tasks[(job_size+1):length(rt_tasks)]) + agg_runtime
task.i <- max(0, min(which(rt.sum >= bot_runtime))-1)
if (!is.infinite(task.i)) {
length(rt_tasks) <- job_size + task.i + 1
rt_tasks[length(rt_tasks)] <- ifelse(task.i > 0, bot_runtime-rt.sum[task.i],
bot_runtime-agg_runtime)
agg_runtime <- bot_runtime
loop <- FALSE
} else {
agg_runtime <- rt.sum[length(rt.sum)]
}
job_size <- length(rt_tasks)
}
ntasks <- c(ntasks,job_size)
return(rt_tasks)
}
uid <- paste("U",u.i,sep="")
pid <- paste("P",p.i,sep="")
iat.cid <- paste("C",iat.clusters[cl.i],sep="")
rtsum.cid <- paste("C",rtsum.clusters[cl.i],sep="")
rttask.cid <- paste("C", trt.clusters[cl.i],sep="")
if (reduced.wl) {
wl <- subset(data.frame(SubmitTime=round(st), RunTime.sum=round(rtsum),
JobID=1:length(ntasks), BoTSize=ntasks, UserID=uid, PeerID=pid,
TraceID=trace, Cluster.IAT=iat.cid, Cluster.JRT=rtsum.cid,
Cluster.TRT=rttask.cid), SubmitTime <= duration.secs)
} else {
wl <- subset(data.frame(SubmitTime=rep(round(st),ntasks), RunTime=round(all_rt_tasks),
JobID=rep(1:length(ntasks),ntasks), UserID=uid, PeerID=pid,
TraceID=trace, Cluster.IAT=iat.cid, Cluster.JRT=rtsum.cid,
Cluster.TRT=rttask.cid), SubmitTime <= duration.secs)
}
write.table(wl, paste('workload_clust_', sites.per.trace, 'spt_', users.per.site, "ups_",
trace,'.txt',sep=""), col.names=is.first, append=!is.first,
quote=FALSE, row.names=FALSE)
is.first <- FALSE
}
}
}
}
GenerateWorkload()
#################################################################################################
#
# Tools for the synthetic workload generation process proposed in the paper:
#
# - Marcus Carvalho, Francisco Brasileiro.
#   Modelagem da Carga de Trabalho de Grades Computacionais Baseada no Comportamento dos UsuÃ¡rios.
#   Submitted to: SimpÃ³sio Brasileiro de Redes de Computadores e Sistemas DistribuÃ­dos (SBRC), 2012.
#
# Last update: 05-Dec-2011
#
#################################################################################################
##
# Generates the synthetic workload using probability distributions listed in the input file.
#
# Inputs:
# - file.dits: input file generated by the model extraction process, with a list of distributions
#              for each attribute and trace.
# - k: number of clusters (groups) for each attribute
# - sites.per.trace: number of sites to create in the synthetic workload for each trace used
#                    in the model
# - users.per.trace: number os users that each created site comprises.
# - duration.sec: the duration of the synthetic workload in seconds.
# - rt.max: limit for the maximum runtime for a job (sum of tasks runtime for a job)
# - reduced.wl: if true, generates a job level workload ommiting task level information. Used if
#               a lower storage footprint is needed and high level information is enough.
# - transform.f: transformation function on the data generated. For instance, if a base-2 log
#                transformation was used in the modeling phase, a base-2 exponential function
#                is needed in the workload generation.
##
GenerateWorkload <- function(file.dists="./wl_fitdist_best.txt", k=5, sites.per.trace=10,
users.per.site=10, duration.secs=3600*24*5, rt.max=3600*24*5,
reduced.wl=FALSE, transform.f=function(x) 2^x) {
require(foreach)
require(doMC)
registerDoMC()
dists.rfun <- list(normal=rnorm, lognormal=rlnorm, gamma=rgamma, exponential=rexp,
weibull=rweibull)
dists.qfun <- list(normal=qnorm, lognormal=qlnorm, gamma=qgamma, exponential=qexp,
weibull=qweibull)
buffer <- 10
df <- read.table(file.dists, header=T)
df <- subset(df, k == 5)
p.i <- 0
u.i <- 0
for(trace in unique(df$Trace)) {
is.first <- TRUE
iat.dist <- subset(df, stats.attr=="bot_user_iat" & Trace==trace)
iat.clusters <- sample(iat.dist$Cluster, sites.per.trace*users.per.site,
prob=iat.dist$Fraction, replace=TRUE)
rtsum.dist <- subset(df, stats.attr=="bot_runtime_sum" & Trace==trace)
rtsum.clusters <- sample(rtsum.dist$Cluster, sites.per.trace*users.per.site,
prob=rtsum.dist$Fraction, replace=TRUE)
trt.dist <- subset(df, stats.attr=="task_runtime" & Trace==trace)
trt.clusters <- sample(trt.dist$Cluster, sites.per.trace*users.per.site,
prob=trt.dist$Fraction, replace=TRUE)
cl.i <- 0
for(peer in 1:sites.per.trace) {
p.i <- p.i + 1
for(user in 1:users.per.site) {
gc()
cl.i <- cl.i + 1
u.i <- u.i + 1
df2 <- subset(df, Trace == trace)
iat.dist <- subset(df2, stats.attr=="bot_user_iat" & Cluster==iat.clusters[cl.i])
d <- as.character(iat.dist$distribution)
t <- 0
iat <- vector()
st <- vector()
while (t < duration.secs) {
if (d != "exponential")
iat <- c(iat, transform.f(dists.rfun[[d]](buffer, iat.dist$Param1, iat.dist$Param2)))
else
iat <- c(iat, transform.f(dists.rfun[[d]](buffer, iat.dist$Param1)))
st <- cumsum(iat)
t <- st[length(st)]
}
njobs <- length(st)
rtsum.dist <- subset(df2, stats.attr=="bot_runtime_sum" & Cluster==rtsum.clusters[cl.i])
d <- as.character(rtsum.dist$distribution)
rtsum <- vector()
jobs.len <- 0
while (jobs.len < njobs) {
if (d != "exponential")
rtsum <- c(rtsum, transform.f(dists.rfun[[d]](buffer, rtsum.dist$Param1,
rtsum.dist$Param2)))
else
rtsum <- c(rtsum, transform.f(dists.rfun[[d]](buffer, rtsum.dist$Param1)))
rtsum <- rtsum[rtsum <= rt.max]
jobs.len <- length(rtsum)
}
length(rtsum) <- njobs
ntasks <- vector()
rttask.dist <- subset(df2, stats.attr=="task_runtime" & Cluster==trt.clusters[cl.i])
d <- as.character(rttask.dist$distribution)
all_rt_tasks <- foreach(bot_runtime=rtsum, .combine=c) %do% {
agg_runtime <- 0
job_size <- 0
rt_tasks <- vector(mode="numeric")
task.i <- 1
loop <- TRUE
while(loop) {
if (d != "exponential")
rt_tasks <- c(rt_tasks, transform.f(dists.rfun[[d]](buffer, rttask.dist$Param1,
rttask.dist$Param2)))
else
rt_tasks <- c(rt_tasks, transform.f(dists.rfun[[d]](buffer, rttask.dist$Param1)))
rt.sum <- cumsum(rt_tasks[(job_size+1):length(rt_tasks)]) + agg_runtime
task.i <- max(0, min(which(rt.sum >= bot_runtime))-1)
if (!is.infinite(task.i)) {
length(rt_tasks) <- job_size + task.i + 1
rt_tasks[length(rt_tasks)] <- ifelse(task.i > 0, bot_runtime-rt.sum[task.i],
bot_runtime-agg_runtime)
agg_runtime <- bot_runtime
loop <- FALSE
} else {
agg_runtime <- rt.sum[length(rt.sum)]
}
job_size <- length(rt_tasks)
}
ntasks <- c(ntasks,job_size)
return(rt_tasks)
}
uid <- paste("U",u.i,sep="")
pid <- paste("P",p.i,sep="")
iat.cid <- paste("C",iat.clusters[cl.i],sep="")
rtsum.cid <- paste("C",rtsum.clusters[cl.i],sep="")
rttask.cid <- paste("C", trt.clusters[cl.i],sep="")
if (reduced.wl) {
wl <- subset(data.frame(SubmitTime=round(st), RunTime.sum=round(rtsum),
JobID=1:length(ntasks), BoTSize=ntasks, UserID=uid, PeerID=pid,
TraceID=trace, Cluster.IAT=iat.cid, Cluster.JRT=rtsum.cid,
Cluster.TRT=rttask.cid), SubmitTime <= duration.secs)
} else {
wl <- subset(data.frame(SubmitTime=rep(round(st),ntasks), RunTime=round(all_rt_tasks),
JobID=rep(1:length(ntasks),ntasks), UserID=uid, PeerID=pid,
TraceID=trace, Cluster.IAT=iat.cid, Cluster.JRT=rtsum.cid,
Cluster.TRT=rttask.cid), SubmitTime <= duration.secs)
}
write.table(wl, paste('workload_clust_', sites.per.trace, 'spt_', users.per.site, "ups_",
trace,'.txt',sep=""), col.names=is.first, append=!is.first,
quote=FALSE, row.names=FALSE)
is.first <- FALSE
}
}
}
}
GenerateWorkload()
#################################################################################################
#
# Tools for the synthetic workload generation process proposed in the paper:
#
# - Marcus Carvalho, Francisco Brasileiro.
#   Modelagem da Carga de Trabalho de Grades Computacionais Baseada no Comportamento dos UsuÃƒÂ¡rios.
#   Submitted to: SimpÃƒÂ³sio Brasileiro de Redes de Computadores e Sistemas DistribuÃƒ­dos (SBRC), 2012.
#
# Last update: 05-Dec-2011
#
#################################################################################################
##
# Generates the synthetic workload using probability distributions listed in the input file.
#
# Inputs:
# - file.dits: input file generated by the model extraction process, with a list of distributions
#              for each attribute and trace.
# - k: number of clusters (groups) for each attribute
# - sites.per.trace: number of sites to create in the synthetic workload for each trace used
#                    in the model
# - users.per.trace: number os users that each created site comprises.
# - duration.sec: the duration of the synthetic workload in seconds.
# - rt.max: limit for the maximum runtime for a job (sum of tasks runtime for a job)
# - reduced.wl: if true, generates a job level workload ommiting task level information. Used if
#               a lower storage footprint is needed and high level information is enough.
# - transform.f: transformation function on the data generated. For instance, if a base-2 log
#                transformation was used in the modeling phase, a base-2 exponential function
#                is needed in the workload generation.
##
GenerateWorkload <- function(file.dists="C:\\Users\\antonio\\Desktop\\Rastros de Workloads\\wl_fitdist_best.txt", k=5, sites.per.trace=10,
users.per.site=10, duration.secs=3600*24*3, rt.max=3600*24*3,
reduced.wl=FALSE, transform.f=function(x) 2^x) {
require(foreach)
require(doMC)
registerDoMC()
dists.rfun <- list(normal=rnorm, lognormal=rlnorm, gamma=rgamma, exponential=rexp,
weibull=rweibull)
dists.qfun <- list(normal=qnorm, lognormal=qlnorm, gamma=qgamma, exponential=qexp,
weibull=qweibull)
buffer <- 10
df <- read.table(file.dists, header=T)
df <- subset(df, k == 5)
p.i <- 0
u.i <- 0
for(trace in unique(df$Trace)) {
is.first <- TRUE
iat.dist <- subset(df, stats.attr=="bot_user_iat" & Trace==trace)
iat.clusters <- sample(iat.dist$Cluster, sites.per.trace*users.per.site,
prob=iat.dist$Fraction, replace=TRUE)
rtsum.dist <- subset(df, stats.attr=="bot_runtime_sum" & Trace==trace)
rtsum.clusters <- sample(rtsum.dist$Cluster, sites.per.trace*users.per.site,
prob=rtsum.dist$Fraction, replace=TRUE)
trt.dist <- subset(df, stats.attr=="task_runtime" & Trace==trace)
trt.clusters <- sample(trt.dist$Cluster, sites.per.trace*users.per.site,
prob=trt.dist$Fraction, replace=TRUE)
cl.i <- 0
for(peer in 1:sites.per.trace) {
p.i <- p.i + 1
for(user in 1:users.per.site) {
gc()
cl.i <- cl.i + 1
u.i <- u.i + 1
df2 <- subset(df, Trace == trace)
iat.dist <- subset(df2, stats.attr=="bot_user_iat" & Cluster==iat.clusters[cl.i])
d <- as.character(iat.dist$distribution)
t <- 0
iat <- vector()
st <- vector()
while (t < duration.secs) {
if (d != "exponential")
iat <- c(iat, transform.f(dists.rfun[[d]](buffer, iat.dist$Param1, iat.dist$Param2)))
else
iat <- c(iat, transform.f(dists.rfun[[d]](buffer, iat.dist$Param1)))
st <- cumsum(iat)
t <- st[length(st)]
}
njobs <- length(st)
rtsum.dist <- subset(df2, stats.attr=="bot_runtime_sum" & Cluster==rtsum.clusters[cl.i])
d <- as.character(rtsum.dist$distribution)
rtsum <- vector()
jobs.len <- 0
while (jobs.len < njobs) {
if (d != "exponential")
rtsum <- c(rtsum, transform.f(dists.rfun[[d]](buffer, rtsum.dist$Param1,
rtsum.dist$Param2)))
else
rtsum <- c(rtsum, transform.f(dists.rfun[[d]](buffer, rtsum.dist$Param1)))
rtsum <- rtsum[rtsum <= rt.max]
jobs.len <- length(rtsum)
}
length(rtsum) <- njobs
ntasks <- vector()
rttask.dist <- subset(df2, stats.attr=="task_runtime" & Cluster==trt.clusters[cl.i])
d <- as.character(rttask.dist$distribution)
all_rt_tasks <- foreach(bot_runtime=rtsum, .combine=c) %do% {
agg_runtime <- 0
job_size <- 0
rt_tasks <- vector(mode="numeric")
task.i <- 1
loop <- TRUE
while(loop) {
if (d != "exponential")
rt_tasks <- c(rt_tasks, transform.f(dists.rfun[[d]](buffer, rttask.dist$Param1,
rttask.dist$Param2)))
else
rt_tasks <- c(rt_tasks, transform.f(dists.rfun[[d]](buffer, rttask.dist$Param1)))
rt.sum <- cumsum(rt_tasks[(job_size+1):length(rt_tasks)]) + agg_runtime
task.i <- max(0, min(which(rt.sum >= bot_runtime))-1)
if (!is.infinite(task.i)) {
length(rt_tasks) <- job_size + task.i + 1
rt_tasks[length(rt_tasks)] <- ifelse(task.i > 0, bot_runtime-rt.sum[task.i],
bot_runtime-agg_runtime)
agg_runtime <- bot_runtime
loop <- FALSE
} else {
agg_runtime <- rt.sum[length(rt.sum)]
}
job_size <- length(rt_tasks)
}
ntasks <- c(ntasks,job_size)
return(rt_tasks)
}
uid <- paste("U",u.i,sep="")
pid <- paste("P",p.i,sep="")
iat.cid <- paste("C",iat.clusters[cl.i],sep="")
rtsum.cid <- paste("C",rtsum.clusters[cl.i],sep="")
rttask.cid <- paste("C", trt.clusters[cl.i],sep="")
if (reduced.wl) {
wl <- subset(data.frame(SubmitTime=round(st), RunTime.sum=round(rtsum),
JobID=1:length(ntasks), BoTSize=ntasks, UserID=uid, PeerID=pid,
TraceID=trace, Cluster.IAT=iat.cid, Cluster.JRT=rtsum.cid,
Cluster.TRT=rttask.cid), SubmitTime <= duration.secs)
} else {
wl <- subset(data.frame(SubmitTime=rep(round(st),ntasks), RunTime=round(all_rt_tasks),
JobID=rep(1:length(ntasks),ntasks), UserID=uid, PeerID=pid,
TraceID=trace, Cluster.IAT=iat.cid, Cluster.JRT=rtsum.cid,
Cluster.TRT=rttask.cid), SubmitTime <= duration.secs)
}
write.table(wl, paste('workload_clust_', sites.per.trace, 'spt_', users.per.site, "ups_",
trace,'.txt',sep=""), col.names=is.first, append=!is.first,
quote=FALSE, row.names=FALSE)
is.first <- FALSE
}
}
}
}
GenerateWorkload()
